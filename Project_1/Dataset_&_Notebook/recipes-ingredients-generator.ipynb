{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'recipe-ingredients-dataset']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECIPE_INGREDIENTS_PATH = '../input/recipe-ingredients-dataset/'\n",
    "FASTTEXT_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes borrowed from [The Effect of Word Embeddings on Bias](https://www.kaggle.com/nholloway/the-effect-of-word-embeddings-on-bias/data)\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f, position=0))\n",
    "\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in tqdm(word_index.items(), position=0):\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            embedding_matrix[i] = np.random.normal(-1, 1, (1, 300))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data_path, cuisine=True):\n",
    "    \n",
    "    x_rid = []\n",
    "    y_rid = []\n",
    "    i_rid = []\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        rid_list = json.load(f)\n",
    "    \n",
    "        for rid in tqdm(rid_list, position=0):\n",
    "            x_rid.append([ing for ing in rid['ingredients']])\n",
    "            if cuisine:\n",
    "                y_rid.append(rid['cuisine'])\n",
    "            i_rid.append(rid['id'])\n",
    "        \n",
    "    return i_rid, x_rid, y_rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_sequences(ingredients, tokenizer):\n",
    "    tokenizer.fit_on_texts(ingredients)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    x_sequences = []\n",
    "    for items in ingredients:\n",
    "        token_list = tokenizer.texts_to_sequences([items])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_grams = token_list[: i + 1]\n",
    "            x_sequences.append(n_grams)\n",
    "    \n",
    "    return x_sequences, total_words, tokenizer\n",
    "\n",
    "\n",
    "def n_gram_padded(x_sequences):\n",
    "    max_len = max([len(x) for x in x_sequences])\n",
    "    x_sequences = np.array(pad_sequences(x_sequences, maxlen=max_len, padding='pre'))\n",
    "    predictors, label = x_sequences[:, :-1], x_sequences[:, -1]\n",
    "    return predictors, label, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39774/39774 [00:00<00:00, 248700.03it/s]\n",
      "100%|██████████| 9944/9944 [00:00<00:00, 272358.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Number of recipes-ingredients 39774\n",
      "Number of unique ingredients 6714\n",
      "Number of unique recipes 20\n",
      "\n",
      "Test\n",
      "Number of recipes-ingredients 9944\n",
      "Number of unique ingredients 4484\n"
     ]
    }
   ],
   "source": [
    "i_rid_train, x_rid_train, y_rid_train = build_dataset(RECIPE_INGREDIENTS_PATH + 'train.json')\n",
    "i_rid_test, x_rid_test, _ = build_dataset(RECIPE_INGREDIENTS_PATH + 'test.json', cuisine=False)\n",
    "\n",
    "print('Train:')\n",
    "print('Number of recipes-ingredients %d' % (len(i_rid_train)))\n",
    "print('Number of unique ingredients %d' % (len(list(set(x for l in x_rid_train for x in l)))))\n",
    "print('Number of unique recipes %d' % (len(list(set(y_rid_train)))))\n",
    "\n",
    "print()\n",
    "print('Test')\n",
    "print('Number of recipes-ingredients %d' % (len(i_rid_test)))\n",
    "print('Number of unique ingredients %d' % (len(list(set(x for l in x_rid_test for x in l)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes-ingredients 49718\n",
      "Sample: pimentos sweet-pepper dried-oregano olive-oil garlic sharp-cheddar-cheese pepper swiss-cheese provolone-cheese canola-oil mushrooms black-olives sausages\n"
     ]
    }
   ],
   "source": [
    "ingredients = x_rid_train + x_rid_test\n",
    "ingredients = [' '.join([item.replace(' ', '-') for item in items]) for items in ingredients]\n",
    "\n",
    "print('Number of recipes-ingredients %d' % len(ingredients))\n",
    "print('Sample: %s' % ingredients[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors (973867, 140)\n",
      "Labels (973867,)\n",
      "Train: 925173, Test: 48694\n",
      "Max length of sequences 141\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "x_sequences, total_words, tokenizer = n_gram_sequences(ingredients, tokenizer)\n",
    "predictors, labels, max_len = n_gram_padded(x_sequences)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(predictors, labels, test_size=0.05, random_state=42)\n",
    "\n",
    "print('Predictors %s' % str(predictors.shape))\n",
    "print('Labels %s' % str(labels.shape))\n",
    "print('Train: %s, Test: %s' % (len(x_train), len(x_test)))\n",
    "print('Max length of sequences %d' % max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [03:08, 10627.67it/s]\n",
      "100%|██████████| 3184/3184 [00:00<00:00, 168871.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words 3185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fasttext_matrix = build_matrix(tokenizer.word_index, FASTTEXT_PATH)\n",
    "print(\"Number of unique words %d\" % fasttext_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  3] 17\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_len, rnn_units, total_words, embedding_matrix):\n",
    "    inputs = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    # x = keras.layers.Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=True)(inputs)\n",
    "    x = keras.layers.CuDNNGRU(rnn_units, name='gru_1')(x)\n",
    "    outputs = tf.keras.layers.Dense(total_words, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "        optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 140)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 140, 300)          955500    \n",
      "_________________________________________________________________\n",
      "gru_1 (CuDNNGRU)             (None, 100)               120600    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3185)              321685    \n",
      "=================================================================\n",
      "Total params: 1,397,785\n",
      "Trainable params: 442,285\n",
      "Non-trainable params: 955,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_len - 1, 100, total_words, fasttext_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 878914 samples, validate on 46259 samples\n",
      "Epoch 1/10\n",
      "878914/878914 [==============================] - 55s 62us/sample - loss: 3.8551 - acc: 0.3070 - val_loss: 3.3275 - val_acc: 0.3681\n",
      "Epoch 2/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 3.2001 - acc: 0.3821 - val_loss: 3.1624 - val_acc: 0.3870\n",
      "Epoch 3/10\n",
      "878914/878914 [==============================] - 52s 60us/sample - loss: 3.0644 - acc: 0.3984 - val_loss: 3.0719 - val_acc: 0.4005\n",
      "Epoch 4/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.9795 - acc: 0.4093 - val_loss: 3.0209 - val_acc: 0.4065\n",
      "Epoch 5/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.9234 - acc: 0.4158 - val_loss: 2.9873 - val_acc: 0.4120\n",
      "Epoch 6/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.8805 - acc: 0.4211 - val_loss: 2.9639 - val_acc: 0.4164\n",
      "Epoch 7/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.8494 - acc: 0.4247 - val_loss: 2.9494 - val_acc: 0.4183\n",
      "Epoch 8/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.8246 - acc: 0.4274 - val_loss: 2.9382 - val_acc: 0.4219\n",
      "Epoch 9/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.8042 - acc: 0.4300 - val_loss: 2.9267 - val_acc: 0.4218\n",
      "Epoch 10/10\n",
      "878914/878914 [==============================] - 52s 59us/sample - loss: 2.7878 - acc: 0.4317 - val_loss: 2.9221 - val_acc: 0.4220\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(x_train, y_train, validation_split=0.05, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, next_words, max_len, model, tokenizer):\n",
    "    idx2word = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    x_pred = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    x_pred = np.array(pad_sequences([x_pred], maxlen=max_len - 1, padding='pre'))\n",
    "    \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(next_words):\n",
    "        predictions = model.predict(x_pred, verbose=0)[0]\n",
    "        predicted_id = sample(predictions, temperature)\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "    \n",
    "    return seed_text + ' ' + ' '.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pimentos sweet-pepper dried-oregano olive-oil chopped bacon italian salt chopped\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"pimentos sweet-pepper dried-oregano olive-oil\", 5, max_len, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
